{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多头注意力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 reshape与view是怎么进行的?  \n",
    "先将原张量从shape的最后一个维度排成一行,然后再按照reshape与view中的参数来排,也是一行一行排的.flatten也是这样做的.   \n",
    "<img src = \"reshape与view是怎么操作的.png\">  \n",
    "### 1.2 transpose与permute是怎么进行的?\n",
    "而transpose与permute是变换维度,其中transpose是一次只能变两个维度,而permute是一次可以变多个维度.我们以transpose为例说一下维度是如何变换的.   \n",
    "如transpose(1, 2),这是交换维度1与维度2,具体交换维度方式为,原矩阵的第(i, j, k, m)位置的值放到新矩阵的第(i, k, j, m)位置."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 self-attention\n",
    "$1$ signal_head_self-attention中每个向量的输入与输出的长度是一样的，所以在单头的self-attention中v的长度就是b的长度，因为b是由v乘注意力分数，而注意力分数是一个标量。一般来说qkv的长度是一样的,也可以设为不一样.     \n",
    "$2$ multi_head_self-attention中每个向量的输入与输出的长度是一样的，而每个输入的n个头是在列上concate的，得到b1，然后将每个输入得到的b1按行concate就得到了总的b，在对b进行乘一个w矩阵就得到最后的B。如下图所示。\n",
    "<center>\n",
    "    <img src = \"多头注意力.png\">\n",
    "<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Attention(nn.Module):\n",
    "    \"\"\"\n",
    "        注意力函数,可以通过参数指定是多头还是单头,注意力评分函数用的是Dot-Product.\n",
    "\n",
    "        Parameters:\n",
    "            toekn_size:每个token向量的长度.\n",
    "            qk_size:每个q与k向量的长度.\n",
    "            v_size:每个v向量的长度.当已有一个头时,qkv三向量的长度是一样的.多头时一般qkv长度也是一样的.\n",
    "                    只要保证最后输出token的长度与输入token的长度一样的就可以.\n",
    "            head_num:有几个头.\n",
    "\n",
    "        Returns:\n",
    "    \"\"\"\n",
    "    def __init__(self, token_size, qk_size, v_size, head_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_size = token_size\n",
    "        self.qk_size = qk_size\n",
    "        self.v_size = v_size\n",
    "        self.head_num = head_num\n",
    "\n",
    "        # 生成qkv的全连接层。\n",
    "        self.W_q = nn.Linear(token_size, qk_size * head_num)\n",
    "        self.W_k = nn.Linear(token_size, qk_size * head_num)\n",
    "        self.W_v = nn.Linear(token_size, v_size * head_num)\n",
    "        self.scale = 1 / torch.sqrt(torch.tensor(qk_size))\n",
    "\n",
    "        # 如果是多头的话，最后还有一个可学习参数矩阵。\n",
    "        self.W = nn.Linear(v_size * head_num, token_size)\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "        注意力函数,可以通过参数指定是多头还是单头,注意力评分函数用的是Dot-Product.\n",
    "\n",
    "        Parameters:\n",
    "            x:形状为(batch, token数量, 每个token的长度)\n",
    "\n",
    "        Returns:\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        batch, token_num, token_size = x.shape\n",
    "        assert self.token_size == token_size, \"判断类参数token长度与输入变量token长度不一样\"\n",
    "\n",
    "        # 变换维度为(batch数, token数, head头数, qkv向量的长度)\n",
    "        # 又transpose交换维度后变为(batch数, head头数, token数, qkv向量长度)\n",
    "        q = self.W_q(x).contiguous().view(batch, token_num, self.head_num, self.qk_size).transpose(1, 2)\n",
    "        k = self.W_k(x).contiguous().view(batch, token_num, self.head_num, self.qk_size).transpose(1, 2)\n",
    "        v = self.W_v(x).contiguous().view(batch, token_num, self.head_num, self.v_size).transpose(1, 2)\n",
    "\n",
    "        # 得到相似度得分\n",
    "        score = torch.matmul(q, k.transpose(2, 3)) * self.scale\n",
    "        score = torch.softmax(score, dim = -1) # (batch, head头数, token数, token数)\n",
    "\n",
    "        # 得到多头的b,维度为(batch, head头数, token数, v向量的长度)\n",
    "        # transpose(1, 2)之后维度为(batch, token数, head头数, v向量的长度)\n",
    "        b_t_h = torch.matmul(score, v).transpose(1, 2)\n",
    "        \n",
    "        # 将维度变为(bathch, token数, head头数 * v向量长度)\n",
    "        b_concate = b_t_h.contiguous().view(batch, token_num, self.head_num * self.v_size)\n",
    "\n",
    "        # 输出的维度为(batch, token数, b向量的长度)\n",
    "        b = self.W(b_concate)\n",
    "\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2756, -0.1439, -0.0618,  ...,  0.1857, -0.0534,  0.5218],\n",
       "         [-0.2755, -0.1445, -0.0611,  ...,  0.1868, -0.0537,  0.5219],\n",
       "         [-0.2750, -0.1441, -0.0622,  ...,  0.1864, -0.0537,  0.5213],\n",
       "         ...,\n",
       "         [-0.2745, -0.1438, -0.0622,  ...,  0.1866, -0.0539,  0.5214],\n",
       "         [-0.2750, -0.1440, -0.0618,  ...,  0.1857, -0.0536,  0.5214],\n",
       "         [-0.2750, -0.1443, -0.0617,  ...,  0.1866, -0.0535,  0.5214]],\n",
       "\n",
       "        [[-0.2555, -0.1716, -0.0731,  ...,  0.2135, -0.0547,  0.5165],\n",
       "         [-0.2551, -0.1718, -0.0727,  ...,  0.2143, -0.0549,  0.5170],\n",
       "         [-0.2545, -0.1710, -0.0725,  ...,  0.2141, -0.0553,  0.5170],\n",
       "         ...,\n",
       "         [-0.2559, -0.1721, -0.0730,  ...,  0.2139, -0.0545,  0.5163],\n",
       "         [-0.2551, -0.1717, -0.0722,  ...,  0.2144, -0.0552,  0.5171],\n",
       "         [-0.2547, -0.1716, -0.0724,  ...,  0.2134, -0.0549,  0.5165]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((2, 128, 100))\n",
    "multi_head_att = Self_Attention(x.shape[2], 2, 2, 12)\n",
    "b = multi_head_att(x)\n",
    "b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
